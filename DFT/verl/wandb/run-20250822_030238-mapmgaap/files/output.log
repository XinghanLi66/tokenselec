len of train dataloader: 58
Total training steps: 116
Epoch 1/2:   0%|          | 0/58 [00:00<?, ?it/s]
Rewards: tensor([[ 0.7122],
        [ 0.7122],
        [-1.4042],
        [ 0.7122],
        [-1.4042],
        [ 0.7122],
        [-1.4042],
        [-1.4042],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [ 0.7122],
        [-1.4042],
        [-1.4042],
        [-1.4042],
        [-1.4042],
        [-1.4042],
        [-1.4042],
        [ 0.7122],
        [ 0.7122],
        [-1.4042],
        [ 0.7122],
        [-1.4042],
        [ 0.7122],
        [ 0.7122]], device='cuda:0', dtype=torch.float64)
Error executing job with overrides: ['data.train_files=data/pi1/pi1_r128_pm_responses_16000.parquet', 'data.val_files=data/pi1/pi1_r128_pm_responses_16000_valid.parquet', 'data.max_length=1920', 'data.prompt_key=prompt', 'data.response_key=response', 'data.train_batch_size=256', 'data.micro_batch_size_per_gpu=32', 'model.partial_pretrain=/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'trainer.default_local_dir=/local1/lxh/save/offline_grpo/1.5b_pi1_ofrl', 'trainer.project_name=dft', 'trainer.experiment_name=1.5b_pi1_ofrl_0822-0302', 'trainer.total_epochs=2', 'trainer.test_freq=5', 'trainer.save_freq=10', 'trainer.logger=[console,wandb]', 'trainer.default_hdfs_dir=null']
Traceback (most recent call last):
  File "/homes/gws/lxh22/rl-sft/DFT/verl/verl/trainer/fsdp_ofrl_trainer.py", line 695, in main
    run_sft(config)
  File "/homes/gws/lxh22/rl-sft/DFT/verl/verl/trainer/fsdp_ofrl_trainer.py", line 687, in run_sft
    trainer.fit()
  File "/homes/gws/lxh22/rl-sft/DFT/verl/verl/trainer/fsdp_ofrl_trainer.py", line 622, in fit
    metric = self.training_step(data)
  File "/homes/gws/lxh22/rl-sft/DFT/verl/verl/trainer/fsdp_ofrl_trainer.py", line 489, in training_step
    loss, original_loss = self._compute_loss_and_backward(batch=micro_batch)
  File "/homes/gws/lxh22/rl-sft/DFT/verl/verl/trainer/fsdp_ofrl_trainer.py", line 370, in _compute_loss_and_backward
    shift_logits = logits[..., :-1, :].contiguous()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.38 GiB. GPU 0 has a total capacity of 44.43 GiB of which 16.23 GiB is free. Process 3588682 has 306.00 MiB memory in use. Process 3588681 has 306.00 MiB memory in use. Process 3588686 has 306.00 MiB memory in use. Process 3588680 has 306.00 MiB memory in use. Process 3588683 has 306.00 MiB memory in use. Process 3588684 has 306.00 MiB memory in use. Process 3588685 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 26.06 GiB memory in use. Of the allocated memory 24.22 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
